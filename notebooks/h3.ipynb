{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ качества и очистка датасета мошеннических финансовых операций\n",
    "\n",
    "Данный ноутбук содержит:\n",
    "1. Инициализацию Spark\n",
    "2. Загрузку и первичный анализ данных\n",
    "3. Выявление проблем с качеством данных\n",
    "4. Очистку данных\n",
    "5. Сохранение очищенного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Инициализация Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransactionDataCleaning\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- tx_datetime: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- terminal_id: long (nullable = true)\n",
      " |-- tx_amount: double (nullable = true)\n",
      " |-- tx_time_seconds: long (nullable = true)\n",
      " |-- tx_time_days: long (nullable = true)\n",
      " |-- tx_fraud: integer (nullable = true)\n",
      " |-- tx_fraud_scenario: integer (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|transaction_id|tx_datetime        |customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|null          |null               |null       |null       |null     |null           |null        |null    |null             |\n",
      "|1832792610    |2022-11-04 14:22:18|0          |53         |63.58    |101139738      |1170        |0       |0                |\n",
      "|1832792611    |2022-11-04 02:12:24|0          |53         |92.95    |101095944      |1170        |0       |0                |\n",
      "|1832792612    |2022-11-04 12:49:35|3          |205        |48.88    |101134175      |1170        |0       |0                |\n",
      "|1832792613    |2022-11-04 02:40:01|5          |383        |24.69    |101097601      |1170        |0       |0                |\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Row count: 46998984\n",
      "\n",
      "Transactions per fraud status:\n",
      "+--------+--------+\n",
      "|tx_fraud|   count|\n",
      "+--------+--------+\n",
      "|    null|       1|\n",
      "|       1| 1406151|\n",
      "|       0|45592832|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdfs_path = \"/user/ubuntu/data/2022-11-04.txt\"\n",
    "\n",
    "# Define schema matching your data structure\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", LongType(), True),\n",
    "    StructField(\"tx_datetime\", StringType(), True),\n",
    "    StructField(\"customer_id\", LongType(), True),\n",
    "    StructField(\"terminal_id\", LongType(), True),\n",
    "    StructField(\"tx_amount\", DoubleType(), True),\n",
    "    StructField(\"tx_time_seconds\", LongType(), True),\n",
    "    StructField(\"tx_time_days\", LongType(), True),\n",
    "    StructField(\"tx_fraud\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud_scenario\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDataAnalysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file with specific options\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/user/ubuntu/data/2022-11-04.txt\")\n",
    "\n",
    "# Verify the data loaded correctly\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nRow count:\", df.count())\n",
    "\n",
    "# Sample analysis\n",
    "print(\"\\nTransactions per fraud status:\")\n",
    "df.groupBy(\"tx_fraud\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Анализ пропущенных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null Value Analysis:\n",
      "Total rows in dataset: 46,998,984\n",
      "\n",
      "Null statistics by column:\n",
      "+-----------------+----------+---------------+\n",
      "|column_name      |null_count|null_percentage|\n",
      "+-----------------+----------+---------------+\n",
      "|tx_fraud         |1         |0.0            |\n",
      "|transaction_id   |1         |0.0            |\n",
      "|tx_amount        |1         |0.0            |\n",
      "|tx_time_seconds  |1         |0.0            |\n",
      "|tx_datetime      |1         |0.0            |\n",
      "|tx_time_days     |1         |0.0            |\n",
      "|customer_id      |1         |0.0            |\n",
      "|terminal_id      |2299      |0.0            |\n",
      "|tx_fraud_scenario|1         |0.0            |\n",
      "+-----------------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_nulls(df):\n",
    "    \"\"\"\n",
    "    Analyze null values in a Spark DataFrame and return statistics\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame to analyze\n",
    "    Returns:\n",
    "        Spark DataFrame with null analysis results\n",
    "    \"\"\"\n",
    "    # Get total number of rows\n",
    "    total_rows = df.count()\n",
    "\n",
    "    # Create expressions for null analysis\n",
    "    null_analytics = []\n",
    "    for col_name in df.columns:\n",
    "        null_analytics.extend([\n",
    "            F.sum(F.when(F.col(col_name).isNull(), 1).otherwise(0)).alias(f\"{col_name}_null_count\"),\n",
    "            F.round((F.sum(F.when(F.col(col_name).isNull(), 1).otherwise(0)) / total_rows) * 100, 2).alias(f\"{col_name}_null_percentage\")\n",
    "        ])\n",
    "\n",
    "    # Calculate all metrics in a single pass\n",
    "    null_analysis = df.select(null_analytics)\n",
    "\n",
    "    # Convert wide format to long format for better readability\n",
    "    result_rows = []\n",
    "    for col_name in df.columns:\n",
    "        row = null_analysis.select(\n",
    "            f\"{col_name}_null_count\",\n",
    "            f\"{col_name}_null_percentage\"\n",
    "        ).first()\n",
    "\n",
    "        result_rows.append((\n",
    "            col_name,\n",
    "            int(row[f\"{col_name}_null_count\"]),\n",
    "            float(row[f\"{col_name}_null_percentage\"])\n",
    "        ))\n",
    "\n",
    "    # Create result DataFrame\n",
    "    result_schema = StructType([\n",
    "        StructField(\"column_name\", StringType(), False),\n",
    "        StructField(\"null_count\", IntegerType(), False),\n",
    "        StructField(\"null_percentage\", DoubleType(), False)\n",
    "    ])\n",
    "\n",
    "    result_df = spark.createDataFrame(result_rows, result_schema)\n",
    "\n",
    "    # Show results sorted by null percentage in descending order\n",
    "    result_df = result_df.orderBy(F.col(\"null_percentage\").desc())\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nNull Value Analysis:\")\n",
    "    print(f\"Total rows in dataset: {total_rows:,}\")\n",
    "    print(\"\\nNull statistics by column:\")\n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "null_analysis_df = analyze_nulls(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Анализ дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate records found: 8\n",
      "Found 8 transactions with duplicate IDs\n",
      "\n",
      "Detailed view of duplicate transactions:\n",
      "+--------------+-----+\n",
      "|transaction_id|count|\n",
      "+--------------+-----+\n",
      "|1862497332    |2    |\n",
      "|1862070073    |2    |\n",
      "|1871745328    |2    |\n",
      "|1843366603    |2    |\n",
      "|1838630470    |2    |\n",
      "|1839244215    |2    |\n",
      "|1865868785    |2    |\n",
      "|1849268479    |2    |\n",
      "+--------------+-----+\n",
      "\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|transaction_id|tx_datetime        |customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|1838630470    |2022-11-07 18:41:37|726029     |145        |19.78    |101414497      |1173        |0       |0                |\n",
      "|1838630470    |2022-11-07 18:41:37|726029     |145        |19.78    |101414497      |1173        |0       |0                |\n",
      "|1839244215    |2022-11-08 04:52:33|118317     |882        |12.84    |101451153      |1174        |0       |0                |\n",
      "|1839244215    |2022-11-08 04:52:33|118317     |882        |12.84    |101451153      |1174        |0       |0                |\n",
      "|1843366603    |2022-11-10 12:14:09|749945     |577        |32.16    |101650449      |1176        |0       |0                |\n",
      "|1843366603    |2022-11-10 12:14:09|749945     |577        |32.16    |101650449      |1176        |0       |0                |\n",
      "|1849268479    |2022-11-14 20:03:36|516951     |958        |45.8     |102024216      |1180        |0       |0                |\n",
      "|1849268479    |2022-11-14 20:03:36|516951     |958        |45.8     |102024216      |1180        |0       |0                |\n",
      "|1862070073    |2022-11-22 11:34:21|687887     |91         |16.55    |102684861      |1188        |0       |0                |\n",
      "|1862070073    |2022-11-22 11:34:21|687887     |91         |16.55    |102684861      |1188        |0       |0                |\n",
      "|1862497332    |2022-11-22 10:25:58|960807     |115        |93.03    |102680758      |1188        |1       |2                |\n",
      "|1862497332    |2022-11-22 10:25:58|960807     |115        |93.03    |102680758      |1188        |1       |2                |\n",
      "|1865868785    |2022-11-25 09:01:27|113752     |939        |77.4     |102934887      |1191        |0       |0                |\n",
      "|1865868785    |2022-11-25 09:01:27|113752     |939        |77.4     |102934887      |1191        |0       |0                |\n",
      "|1871745328    |2022-11-28 15:40:09|864034     |518        |128.76   |103218009      |1194        |0       |0                |\n",
      "|1871745328    |2022-11-28 15:40:09|864034     |518        |128.76   |103218009      |1194        |0       |0                |\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total duplicates (all columns)\n",
    "total_duplicates = df.count() - df.dropDuplicates().count()\n",
    "print(f\"Total duplicate records found: {total_duplicates}\")\n",
    "\n",
    "# Analyze duplicates by transaction_id\n",
    "transaction_duplicates = df.groupBy(\"transaction_id\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .filter(\"count > 1\")\n",
    "\n",
    "# Get count of duplicate transactions\n",
    "duplicate_count = transaction_duplicates.count()\n",
    "print(f\"Found {duplicate_count} transactions with duplicate IDs\")\n",
    "\n",
    "# Optional: Show detailed information about duplicates\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nDetailed view of duplicate transactions:\")\n",
    "    transaction_duplicates.show(truncate=False)\n",
    "\n",
    "    # Show full records for duplicated transaction_ids\n",
    "    duplicate_ids = transaction_duplicates.select(\"transaction_id\")\n",
    "    df.join(duplicate_ids, \"transaction_id\") \\\n",
    "        .orderBy(\"transaction_id\") \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Анализ выбросов в числовых полях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Колонка: tx_amount\n",
      "Количество выбросов: 724473\n",
      "Процент выбросов: 1.54%\n",
      "\n",
      "Колонка: tx_fraud\n",
      "Количество выбросов: 1406151\n",
      "Процент выбросов: 2.99%\n",
      "\n",
      "Колонка: tx_fraud_scenario\n",
      "Количество выбросов: 1406151\n",
      "Процент выбросов: 2.99%\n"
     ]
    }
   ],
   "source": [
    "def analyze_outliers(df, numeric_cols):\n",
    "    for col in numeric_cols:\n",
    "        # Вычисляем квартили\n",
    "        quantiles = df.approxQuantile(col, [0.25, 0.5, 0.75], 0.01)\n",
    "        Q1, median, Q3 = quantiles\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Определяем границы выбросов\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Подсчитываем выбросы\n",
    "        outliers = df.filter(\n",
    "            (F.col(col) < lower_bound) |\n",
    "            (F.col(col) > upper_bound)\n",
    "        ).count()\n",
    "\n",
    "        print(f\"\\nКолонка: {col}\")\n",
    "        print(f\"Количество выбросов: {outliers}\")\n",
    "        print(f\"Процент выбросов: {(outliers/df.count())*100:.2f}%\")\n",
    "\n",
    "# Определяем числовые колонки\n",
    "numeric_columns = [field.name for field in df.schema.fields\n",
    "                  if isinstance(field.dataType, (IntegerType, DoubleType, FloatType))]\n",
    "\n",
    "analyze_outliers(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Анализ категориальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Колонка: tx_datetime\n",
      "Количество уникальных значений: 2585594\n",
      "Пустые строки: 0\n",
      "Записи со спец. символами: 46998983\n",
      "\n",
      "Топ-10 значений:\n",
      "+-------------------+-----+\n",
      "|tx_datetime        |count|\n",
      "+-------------------+-----+\n",
      "|2022-11-13 14:02:36|62   |\n",
      "|2022-11-19 10:52:56|61   |\n",
      "|2022-11-10 12:09:02|59   |\n",
      "|2022-11-29 12:28:29|59   |\n",
      "|2022-11-09 12:13:32|58   |\n",
      "|2022-11-23 11:12:13|58   |\n",
      "|2022-11-05 10:16:19|57   |\n",
      "|2022-11-21 10:54:05|57   |\n",
      "|2022-11-06 10:22:27|57   |\n",
      "|2022-11-24 11:47:16|57   |\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def analyze_categorical(df, categorical_cols):\n",
    "    for col in categorical_cols:\n",
    "        # Получаем топ-10 значений и их частоты\n",
    "        value_counts = df.groupBy(col).count().orderBy('count', ascending=False)\n",
    "\n",
    "        # Проверяем на пустые строки и специальные символы\n",
    "        empty_strings = df.filter(F.trim(F.col(col)) == \"\").count()\n",
    "        # Используем rlike вместо regexp\n",
    "        special_chars = df.filter(F.col(col).rlike('[^a-zA-Z0-9\\\\s]')).count()\n",
    "\n",
    "        print(f\"\\nКолонка: {col}\")\n",
    "        print(f\"Количество уникальных значений: {value_counts.count()}\")\n",
    "        print(f\"Пустые строки: {empty_strings}\")\n",
    "        print(f\"Записи со спец. символами: {special_chars}\")\n",
    "\n",
    "        # Выводим топ-10 значений\n",
    "        print(\"\\nТоп-10 значений:\")\n",
    "        value_counts.show(10, truncate=False)\n",
    "\n",
    "# Определяем категориальные колонки\n",
    "categorical_columns = [field.name for field in df.schema.fields\n",
    "                      if isinstance(field.dataType, StringType)]\n",
    "\n",
    "analyze_categorical(df, categorical_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
